{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ min \\ f = \\sum_{j=1}^K \\sum_{x_{i}\\epsilon C_{j}} || x_{j}-\\mu_{j}||^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "class Particle:\n",
    "    def __init__(self, num_clusters, num_features):\n",
    "        self.position = np.random.rand(num_clusters, num_features)\n",
    "        self.velocity = np.random.rand(num_clusters, num_features)\n",
    "        self.best_position = self.position.copy()\n",
    "        self.best_fitness = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_function(data, particles, num_clusters):\n",
    "    distances = np.zeros((len(data), num_clusters))\n",
    "\n",
    "    for i, particle in enumerate(particles):\n",
    "        for j in range(num_clusters):\n",
    "            distances[:, j] = np.linalg.norm(data - particle.position[j], axis=1)\n",
    "\n",
    "        particle_fitness = np.sum(np.min(distances, axis=1))\n",
    "        \n",
    "        if particle_fitness < particle.best_fitness:\n",
    "            particle.best_fitness = particle_fitness\n",
    "            particle.best_position = particle.position.copy()\n",
    "\n",
    "    return particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_velocity_position(particles, inertia, c1, c2, global_best_position):\n",
    "    for particle in particles:\n",
    "        inertia_term = inertia * particle.velocity\n",
    "        cognitive_term = c1 * np.random.rand() * (particle.best_position - particle.position)\n",
    "        social_term = c2 * np.random.rand() * (global_best_position - particle.position)\n",
    "\n",
    "        particle.velocity = inertia_term + cognitive_term + social_term\n",
    "        particle.position = particle.position + particle.velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_particles(num_particles, num_clusters, num_features):\n",
    "    particles = [Particle(num_clusters, num_features) for _ in range(num_particles)]\n",
    "    return particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data and set parameters\n",
    "data = np.loadtxt(\"./data/wikipedia_td.txt\")\n",
    "num_particles = 30\n",
    "num_clusters = 10\n",
    "num_features = data.shape[1]\n",
    "max_iterations = 100\n",
    "inertia = 0.5\n",
    "c1 = 2.0\n",
    "c2 = 2.0\n",
    "\n",
    "\n",
    "# Initialize particles\n",
    "particles = initialize_particles(num_particles, num_clusters, num_features)\n",
    "global_best_position = None\n",
    "global_best_fitness = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSO Main Loop\n",
    "for iteration in range(max_iterations):\n",
    "    particles = fitness_function(data, particles, num_clusters)\n",
    "    \n",
    "    # Update global best\n",
    "    for particle in particles:\n",
    "        if particle.best_fitness < global_best_fitness:\n",
    "            global_best_fitness = particle.best_fitness\n",
    "            global_best_position = particle.best_position.copy()\n",
    "\n",
    "    # Update particles' velocity and position\n",
    "    update_velocity_position(particles, inertia, c1, c2, global_best_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words for each cluster:\n",
      "Cluster 1: raise, mark, associate, focus, base, claim, network, maintain, statement, track\n",
      "Cluster 2: california, former, occur, enter, capture, learn, east, dead, date, week\n",
      "Cluster 3: response, website, cover, front, marriage, job, appearance, range, contain, arrive\n",
      "Cluster 4: approach, version, move, college, question, request, premier, establish, paul, class\n",
      "Cluster 5: program, approach, hour, artist, question, college, result, decide, washington, injury\n",
      "Cluster 6: theme, suffer, pay, decline, earlier, british, boy, special, shortly, true\n",
      "Cluster 7: promote, confirm, develop, sister, dead, theme, kingdom, level, argue, respond\n",
      "Cluster 8: organization, common, heart, half, result, publish, finish, feel, land, hospital\n",
      "Cluster 9: victory, sister, middle, top, stage, image, lack, initial, town, board\n",
      "Cluster 10: originally, respectively, feature, fire, popular, broadcast, john, require, hope, strike\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Load dictionary from CSV file\n",
    "dictionary_path = \"./data/dictionary.csv\"  # Replace with the actual path to your dictionary CSV file\n",
    "dictionary = {}\n",
    "\n",
    "with open(dictionary_path, \"r\", encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for index, word in enumerate(reader):\n",
    "        dictionary[index] = word[0]\n",
    "\n",
    "# print(dictionary)\n",
    "\n",
    "# Print the words corresponding to the top 10 entries of the global best position\n",
    "sorted_indices = np.argsort(-global_best_position, axis=1)\n",
    "top_words = [[dictionary[idx] for idx in row[:10]] for row in sorted_indices]\n",
    "print(\"Top words for each cluster:\")\n",
    "for cluster_index, words in enumerate(top_words):\n",
    "    print(f\"Cluster {cluster_index + 1}: {', '.join(words)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Unfinished_portrait_of_Franklin_D._Roosevelt', 1: 'Negan', 2: 'Cam_Newton', 3: 'Beyonce', 4: 'Coachella_Valley_Music_and_Arts_Festival', 5: 'Charlie_Sheen', 6: 'Keanu_Reeves', 7: 'Crimson_Peak', 8: 'Lisa_Brennan-Jobs', 9: 'Rodrigo_Duterte', 10: 'Conor_McGregor', 11: 'The_Life_of_Pablo', 12: 'Memorial_Day', 13: 'Veterans_Day', 14: 'Labor_Day', 15: 'UEFA_Euro_2016', 16: 'Ides_of_March', 17: 'Pat_Bowlen', 18: 'The_Martian_(film)', 19: 'David_Bowie', 20: 'Hanukkah', 21: '2016_ICC_World_Twenty20', 22: 'Whitey_Bulger', 23: 'Chris_Stapleton', 24: 'House_of_Cards_(season_4)', 25: 'Ted_Cruz', 26: 'Clara_Rockmore', 27: 'Flip_Saunders', 28: 'Channing_Tatum', 29: 'Meldonium', 30: 'Fear_the_Walking_Dead', 31: 'Terry_Wogan', 32: 'Negasonic_Teenage_Warhead', 33: 'Leap_year', 34: 'Good_Friday', 35: 'Mahatma_Gandhi', 36: 'Michael_J._Fox', 37: 'Lupe_Fuentes', 38: 'Lucy_(Australopithecus)', 39: 'Von_Miller', 40: 'Blac_Chyna', 41: 'Prince_(musician)', 42: 'The_Walk_(2015_film)', 43: 'Tami_Erin', 44: 'Back_to_the_Future_II', 45: 'Zika_virus', 46: 'Doom_(2016_video_game)', 47: 'Preacher_(comics)', 48: 'Fan_(film)', 49: 'Daniel_Craig', 50: 'Democratic_Party_presidential_primaries,_2016', 51: '2015_Rugby_World_Cup', 52: 'Vanity_(singer)', 53: 'B._K._S._Iyengar', 54: 'Claude_Shannon', 55: 'Natalie_Cole', 56: 'Cedric_Villani', 57: 'Rai_stones', 58: 'Hillsborough_disaster', 59: 'Groundhog_Day', 60: 'Novak_Djokovic', 61: 'Spotlight_(film)', 62: 'Purple_Rain_(film)', 63: \"2016_NCAA_Men's_Division_I_Basketball_Tournament\", 64: 'Dr._Luke', 65: 'Kobe_Bryant', 66: 'Jurgen_Klopp', 67: 'Sri_Srinivasan', 68: 'Joy_Mangano', 69: 'November_2015_Paris_attacks', 70: '1896_Summer_Olympics', 71: 'Verghese_Kurien', 72: 'Hamilton_(musical)', 73: 'Blackstar_(David_Bowie_album)', 74: 'Roger_Federer', 75: 'Joe_Frazier', 76: 'Glenn_Frey', 77: 'Robert_Hanssen', 78: 'House_of_Cards_(U.S._TV_series)', 79: 'Doctor_Strange', 80: 'Jessica_Jones_(TV_series)', 81: 'Seventh-day_Adventist_Church', 82: 'William_Shakespeare', 83: 'Star_Wars_Episode_I:_The_Phantom_Menace', 84: 'Eagles_(band)', 85: 'Laila_Ali', 86: '2016_in_film', 87: 'The_Walking_Dead_(season_6)', 88: 'Drake_(rapper)', 89: 'Emilia_Clarke', 90: 'Singh_Is_Bliing', 91: 'Syrian_Civil_War', 92: 'Survivor_Series_(2015)', 93: '10_Cloverfield_Lane', 94: 'Spectre_(2015_film)', 95: 'Marco_Rubio', 96: 'A_Moon_Shaped_Pool', 97: 'Andrew_Jackson', 98: 'Making_a_Murderer', 99: 'Anonymous_(group)', 100: 'Lemonade_(Beyonce_album)', 101: 'Elizabeth_II', 102: '58th_Annual_Grammy_Awards', 103: 'Coldplay', 104: 'Samburu_people', 105: 'Angie_Bowie', 106: 'Martin_Luther_King,_Jr.', 107: 'Johnny_Depp', 108: 'Hate_Story_3', 109: 'Annie_Besant', 110: 'Salford_City_F.C.', 111: 'Anne_Frank', 112: 'Yom_Kippur', 113: '2015_San_Bernardino_shooting', 114: 'Leicester_City_F._C.', 115: 'Chinese_New_Year', 116: 'Supersonic_Low_Altitude_Missile', 117: 'Omayra_Sanchez', 118: 'Canadian_federal_election,_2015', 119: 'Floyd_Mayweather,_Jr.', 120: 'Philippe_Petit', 121: 'Matt_Hasselbeck', 122: 'Henry_Cavill', 123: 'Mohammad_Azharuddin', 124: 'Syria', 125: 'John_Boehner', 126: 'Rosh_Hashanah', 127: 'Super_Bowl', 128: 'Sukhoi_Su-24', 129: 'Black_Friday_(shopping)', 130: 'Hillary_Clinton', 131: 'Ronald_Reagan', 132: 'Antonin_Scalia', 133: 'Dmitri_Mendeleev', 134: '73rd Golden Globe Awards', 135: 'Patty_Duke', 136: 'Bajirao_I', 137: 'Royal_Rumble_(2016)', 138: \"Valentine's_Day\", 139: 'Claudio_Ranieri', 140: 'X-Men_(film_series)', 141: 'Harriet_Tubman', 142: 'Jose_Mourinho', 143: 'Billy_the_Kid', 144: 'Danny_Willett', 145: 'Black_hole', 146: 'Rob_Ford', 147: 'Brussels', 148: 'Jacklyn_H._Lucas', 149: 'Eazy-E', 150: 'Purple_Man', 151: 'Jackie_Robinson', 152: 'American_Horror_Story', 153: 'Ash_Wednesday', 154: 'Virat_Kohli', 155: 'Nancy_Reagan', 156: 'M._F._Husain', 157: 'Bernard_Madoff', 158: 'Elektra_(comics)', 159: 'Mario_Miranda', 160: 'Blood_Moon_Prophecy', 161: 'Halo_5:_Guardians', 162: 'Adam_Driver', 163: 'Suicide_Squad_(film)', 164: 'Diwali', 165: 'Punisher', 166: 'Eid_al-Adha', 167: 'Fort_McMurray', 168: 'Phife_Dawg', 169: 'Marvel_Cinematic_Universe', 170: 'B._R._Ambedkar', 171: 'John_Anglin_(criminal)', 172: 'Return_of_the_Jedi', 173: 'Jordan_Spieth', 174: 'Ron_Reagan', 175: 'Academy_Awards', 176: 'Quantico_(TV_series)', 177: 'Holi', 178: 'TLC:_Tables,_Ladders_&_Chairs_(2015)', 179: 'Rod_Serling', 180: 'Patti_Davis', 181: 'Neerja_Bhanot', 182: 'Sylvester_Stallone', 183: 'Krampus', 184: 'Sigmund_Freud', 185: 'Carolina_Panthers', 186: 'Pablo_Escobar', 187: 'The_Man_in_the_High_Castle_(TV_series)', 188: 'Ben_Carson', 189: 'Mali', 190: 'Trans-Pacific_Partnership', 191: 'Master_of_None', 192: 'Miss_Universe', 193: 'Martin_Luther_King,_Jr._Day', 194: 'Attack_on_Pearl_Harbor', 195: 'A._P._J._Abdul_Kalam', 196: 'Star_Wars_sequel_trilogy', 197: 'Bob_Ross', 198: 'Gloria_Vanderbilt', 199: 'Lemmy', 200: 'Dolly_Parton', 201: 'Jose_Aldo', 202: 'O._J._Simpson', 203: 'Cinco_de_Mayo', 204: 'Super_Tuesday', 205: 'Kanye_West', 206: \"Mother's_Day\", 207: 'Daisy_Ridley', 208: 'The_Ultimate_Fighter:_Team_McGregor_vs._Team_Faber', 209: 'April_Ludgate', 210: 'Belgium', 211: 'Nusrat_Fateh_Ali_Khan', 212: 'Serena_Williams', 213: 'Rockbitch', 214: 'Descendants_of_the_Sun', 215: 'Black_Panther_(comics)', 216: 'Jodie_Sweetin', 217: 'Crypt_of_Civilization', 218: 'Jim_Webb', 219: 'Supergirl_(U.S._TV_series)', 220: 'Payback_(2016)', 221: 'Easy-E', 222: 'Donald_Trump', 223: 'Gennady_Golovkin', 224: 'Duncan_Jones', 225: 'Remembrance_Day', 226: 'Powerball', 227: 'Dilwale_(2015_film)', 228: 'Mastani', 229: 'Heroes_Reborn_(miniseries)', 230: 'Adele', 231: 'Karan_Singh_Grover', 232: 'Grease:_Live', 233: 'Tamasha_(film)', 234: 'Johan_Cruyff', 235: 'Michael_Jordan', 236: 'Lunar_eclipse', 237: 'Christina_Grimmie', 238: 'Ronda_Rousey', 239: 'Andre_the_Giant', 240: 'Azhar_(film)', 241: 'Space_Shuttle_Challenger_disaster', 242: 'Krampus_(film)', 243: 'Jeffrey_Dean_Morgan', 244: 'Prem_Ratan_Dhan_Payo', 245: \"April_Fools'_Day\", 246: 'Call_of_Duty:_Black_Ops_III', 247: 'Ryan_Reynolds', 248: 'Alicia_Vikander', 249: 'Views_(album)', 250: 'Wladimir_Klitschko', 251: 'Shane_McMahon', 252: 'Islamic_State_of_Iraq_and_the_Levant', 253: 'Google', 254: 'Fuller_House_(TV_series)', 255: \"Saint_Patrick's_Day\", 256: 'Eagles_of_Death_Metal', 257: \"Uncharted_4:_A_Thief's_End\", 258: 'Alexander_Hamilton', 259: 'George_Boole', 260: 'Philippine_presidential_election,_2016', 261: 'O._J._Simpson_murder_case', 262: 'Deadpool', 263: 'Stephen_Harper', 264: \"International_Women's_Day\", 265: 'Fastlane_(2016)', 266: 'Jenna_Dewan', 267: 'Anti_(album)', 268: 'Elizabeth_Olsen', 269: 'Robert_Kardashian', 270: 'A._J._Styles', 271: 'Victoria_Woodhull', 272: 'Iowa_caucuses', 273: 'Zootopia', 274: 'Abu_Bakr_al-Baghdadi', 275: 'Yogi_Berra', 276: \"Fool's_Gold_Loaf\", 277: 'Saul_Alvarez', 278: 'Creed_(film)', 279: 'Deadpool_(film)', 280: 'Celine_Dion', 281: 'Hertha_Marks_Ayrton', 282: '2012_Benghazi_attack', 283: 'Ethan_Couch', 284: 'United_States', 285: 'Thanksgiving', 286: 'Victoria_Wood', 287: 'Genie_(feral_child)', 288: 'Holly_Holm', 289: 'Denver_Broncos', 290: 'Eurovision_Song_Contest_2016', 291: 'Steve_Jobs', 292: 'Bluetooth', 293: 'Stacey_Dash', 294: 'Goran_Kropp', 295: 'Nate_Diaz', 296: 'Rogue_One', 297: 'Gotham_(TV_series)', 298: 'American_Crime_Story', 299: 'The_Jungle_Book_(2016_film)', 300: 'Frederick_Douglass', 301: 'Captain_America:_Civil_War', 302: 'Christopher_Columbus', 303: 'Tom_Brady', 304: 'Krysten_Ritter', 305: 'Fallout_4', 306: 'George_Hotz', 307: 'Republican_Party_presidential_primaries,_2016', 308: 'Bernie_Sanders', 309: 'Carly_Fiorina', 310: 'Lady_Colin_Campbell', 311: '2016_Brussels_bombings', 312: 'Sadiq_Khan', 313: 'Garry_Shandling', 314: 'Mayte_Garcia', 315: 'Batman_v_Superman:_Dawn_of_Justice', 316: '2015_FIBA_Asia_Championship', 317: 'Jonah_Lomu', 318: 'Christopher_Lloyd', 319: 'Affluenza', 320: 'Star_Wars', 321: 'Zaha_Hadid', 322: 'UEFA_Euro_2016_qualifying', 323: '88th_Academy_Awards', 324: 'Extreme_Rules_(2016)', 325: 'Mary_Fields', 326: 'Alan_Rickman', 327: 'Caroline_Herschel', 328: 'Luke_Cage', 329: 'Jackie_Collins', 330: '2008_Noida_double_murder_case', 331: 'Charles_Perrault', 332: 'September_11_attacks', 333: 'Hugh_Glass', 334: 'Joy_(film)', 335: 'Game_of_Thrones_(season_6)', 336: 'Bajirao_Mastani_(film)', 337: 'Day_of_the_Dead', 338: 'Narcos', 339: 'Mark_Hamill', 340: 'Daredevil_(TV_series)', 341: 'Gwen_Stefani', 342: 'Iman_(model)', 343: 'Halloween', 344: 'Adolphe_Sax', 345: 'Margaret_Trudeau', 346: 'Abe_Vigoda', 347: 'The_Revenant_(2015_film)', 348: 'Malcolm_Turnbull', 349: 'The_Conjuring_2', 350: 'Airlift_(film)', 351: 'Fidel_Castro', 352: 'Easter', 353: 'Black_Panther_Party', 354: 'Pierre_Trudeau', 355: 'Carol_Burnett', 356: 'Kimbo_Slice', 357: 'Fargo_(TV_series)', 358: '25_(Adele_album)', 359: 'The_Walking_Dead_(TV_series)', 360: 'Lamar_Odom', 361: 'Lucy_Maud_Montgomery', 362: 'Dimitri_Payet', 363: 'Stephen_Curry', 364: 'Room_(2015_film)', 365: 'Hedy_Lamarr', 366: 'Panama_Papers', 367: 'Corrupted_Blood_incident', 368: 'Daredevil_(season_2)', 369: 'Christopher_Paul_Neil', 370: 'Saint_Patrick', 371: 'Great_white_shark', 372: 'Ecole_Polytechnique_massacre', 373: 'Warcraft_(film)', 374: 'Ramadan', 375: 'Gal_Gadot', 376: 'Scott_Weiland', 377: 'Purpose_(Justin_Bieber_album)', 378: 'Earth_Day', 379: 'Goliath', 380: 'Dave_Mirra', 381: 'Daylight_Saving_Time', 382: 'Harrison_Ford', 383: 'Hello_(Adele_song)', 384: 'David_Bowie_discography', 385: 'Joaquin_Guzman', 386: 'Tyson_Fury', 387: 'Doris_Roberts', 388: 'Motorhead', 389: 'Hell_in_a_Cell_(2015)', 390: 'Star_Wars_(film)', 391: 'Ronnie_Corbett', 392: 'Will_Smith_(defensive_end)', 393: 'Jane_Jacobs', 394: 'Martin_Shkreli', 395: 'Sian_Blake', 396: 'Pia_Wurtzbach', 397: 'Rachel_Roy', 398: 'Game_of_Thrones', 399: '2016_UEFA_Champions_League_Final', 400: 'Super_Bowl_50', 401: 'ICC_World_Twenty20', 402: 'Baaghi_(2016_film)', 403: 'Sean_Astin', 404: 'Theri_(film)', 405: 'American_Horror_Story:_Hotel', 406: 'Brie_Larson', 407: 'Frank_Sinatra,_Jr.', 408: 'Chernobyl_disaster', 409: 'Kate_Winslet', 410: 'How_to_Get_Away_with_Murder', 411: 'Sherlock_(TV_series)', 412: 'Sairat', 413: 'Michael_Jackson', 414: 'Steven_Avery', 415: 'Fred_Thompson', 416: 'Moses_Malone', 417: 'Kate_Beckinsale', 418: 'Starship_Troopers_(film)', 419: 'Frankie_Manning', 420: 'Overwatch_(video_game)', 421: 'Amber_Heard', 422: \"Maureen_O'Hara\", 423: 'Facebook', 424: 'Justin_Trudeau', 425: 'Ludwig_van_Beethoven', 426: 'Rene_Angelil', 427: 'Ben_Affleck', 428: 'Scream_Queens_(2015_TV_series)', 429: 'Call_of_Duty:_Modern_Warfare_2', 430: 'Back_to_the_Future', 431: 'Pope_Francis', 432: 'Morley_Safer', 433: 'Yuri_Kochiyama', 434: 'Purple_Rain_(album)', 435: 'One-Punch_Man', 436: 'Monica_Lewinsky', 437: 'Puli_(2015_film)', 438: 'Everest_(2015_film)', 439: 'Ariana_Grande', 440: 'Sia_Furler', 441: 'The_X-Files', 442: 'Guy_Fawkes', 443: 'University_of_Texas_at_Dallas', 444: 'Merrick_Garland', 445: 'H._H._Holmes', 446: 'Merle_Haggard', 447: 'The_Hateful_Eight', 448: 'Wrestlemania_32', 449: 'Jessica_Jones', 450: 'Maria_Santos_Gorrostieta_Salazar', 451: 'Peyton_Manning', 452: 'The_Flash_(2014_TV_series)', 453: 'Guy_Fawkes_Night', 454: 'Boxing_Day', 455: 'Rene_Laennec', 456: 'Steven_Harvey', 457: 'Muhammad_Ali', 458: 'Zach_Braff', 459: 'Phoebe_Snetsinger', 460: 'Mark_Zuckerberg', 461: 'Mad_Max:_Fury_Road', 462: 'Aokigahara', 463: 'Preacher_(TV_series)', 464: 'Jeremy_Corbyn', 465: 'Mars', 466: 'Night_of_Champions_(2015)', 467: 'Disappearance_of_Bobby_Dunbar', 468: 'Tom_Hardy', 469: 'Kylo_Ren', 470: 'Kesha', 471: 'UFC_199', 472: 'Vincent_Margera', 473: 'UFC_192', 474: 'UFC_193', 475: 'UFC_194', 476: 'UFC_196', 477: 'UFC_197', 478: 'J._J._Abrams', 479: 'Copa_America_Centenario', 480: 'Wonder_Woman', 481: 'Melania_Trump', 482: 'Chris_Kyle', 483: 'Leonardo_DiCaprio', 484: 'Amrita_Sher-Gil', 485: 'Villanova_University', 486: 'Ravi_Shankar', 487: 'Eli_Manning', 488: 'X-Men:_Apocalypse', 489: 'John_Logie_Baird', 490: 'The_X-Files_(miniseries)', 491: 'Sophie_Gregoire', 492: 'The_Shannara_Chronicles', 493: 'Cristiano_Ronaldo', 494: 'Star_Wars:_The_Force_Awakens', 495: 'Lyndon_B._Johnson', 496: 'The_Hunger_Games:_Mockingjay_-_Part_2', 497: 'Gordie_Howe', 498: 'Republic_Day_(India)', 499: 'Carrie_Fisher'}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Load dictionary from CSV file\n",
    "titles_path = \"./data/titles.csv\"  # Replace with the actual path to your dictionary CSV file\n",
    "titles = {}\n",
    "\n",
    "with open(titles_path, \"r\", encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for index, title in enumerate(reader):\n",
    "        titles[index] = title[0]\n",
    "print(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 titles for each cluster:\n",
      "Cluster 1: Crimson_Peak, Lisa_Brennan-Jobs, Negan, Cam_Newton, Keanu_Reeves, Lisa_Brennan-Jobs, Cam_Newton, Keanu_Reeves, Negan, Coachella_Valley_Music_and_Arts_Festival\n",
      "Cluster 2: Lisa_Brennan-Jobs, Coachella_Valley_Music_and_Arts_Festival, Charlie_Sheen, Crimson_Peak, Negan, Negan, Negan, Rodrigo_Duterte, Cam_Newton, Lisa_Brennan-Jobs\n",
      "Cluster 3: Rodrigo_Duterte, Unfinished_portrait_of_Franklin_D._Roosevelt, Cam_Newton, Charlie_Sheen, Charlie_Sheen, Keanu_Reeves, Keanu_Reeves, Crimson_Peak, Charlie_Sheen, Charlie_Sheen\n",
      "Cluster 4: Beyonce, Charlie_Sheen, Lisa_Brennan-Jobs, Beyonce, Unfinished_portrait_of_Franklin_D._Roosevelt, Charlie_Sheen, Coachella_Valley_Music_and_Arts_Festival, Charlie_Sheen, Unfinished_portrait_of_Franklin_D._Roosevelt, Crimson_Peak\n",
      "Cluster 5: Keanu_Reeves, Cam_Newton, Rodrigo_Duterte, Unfinished_portrait_of_Franklin_D._Roosevelt, Lisa_Brennan-Jobs, Coachella_Valley_Music_and_Arts_Festival, Beyonce, Beyonce, Keanu_Reeves, Unfinished_portrait_of_Franklin_D._Roosevelt\n",
      "Cluster 6: Charlie_Sheen, Beyonce, Beyonce, Negan, Crimson_Peak, Beyonce, Lisa_Brennan-Jobs, Cam_Newton, Beyonce, Rodrigo_Duterte\n",
      "Cluster 7: Unfinished_portrait_of_Franklin_D._Roosevelt, Keanu_Reeves, Coachella_Valley_Music_and_Arts_Festival, Coachella_Valley_Music_and_Arts_Festival, Rodrigo_Duterte, Unfinished_portrait_of_Franklin_D._Roosevelt, Unfinished_portrait_of_Franklin_D._Roosevelt, Negan, Crimson_Peak, Keanu_Reeves\n",
      "Cluster 8: Negan, Crimson_Peak, Keanu_Reeves, Rodrigo_Duterte, Beyonce, Rodrigo_Duterte, Rodrigo_Duterte, Lisa_Brennan-Jobs, Coachella_Valley_Music_and_Arts_Festival, Negan\n",
      "Cluster 9: Cam_Newton, Negan, Unfinished_portrait_of_Franklin_D._Roosevelt, Lisa_Brennan-Jobs, Coachella_Valley_Music_and_Arts_Festival, Cam_Newton, Charlie_Sheen, Unfinished_portrait_of_Franklin_D._Roosevelt, Lisa_Brennan-Jobs, Beyonce\n",
      "Cluster 10: Coachella_Valley_Music_and_Arts_Festival, Rodrigo_Duterte, Crimson_Peak, Keanu_Reeves, Cam_Newton, Crimson_Peak, Crimson_Peak, Coachella_Valley_Music_and_Arts_Festival, Rodrigo_Duterte, Cam_Newton\n"
     ]
    }
   ],
   "source": [
    "sorted_indices = np.argsort(-global_best_position, axis=0)\n",
    "top_titles = [[titles[idx] for idx in row[:10]] for row in sorted_indices]\n",
    "print(\"Top 10 titles for each cluster:\")\n",
    "for cluster_index, titles in enumerate(top_titles):\n",
    "    print(f\"Cluster {cluster_index + 1}: {', '.join(titles)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 500)\n"
     ]
    }
   ],
   "source": [
    "# Load titles from CSV file\n",
    "titles_path = \"./data/titles.csv\"  # Replace with the actual path to your titles CSV file\n",
    "titles_df = pd.read_csv(titles_path, header=None, names=['Titles'])\n",
    "\n",
    "# Find the top 10 titles for each cluster\n",
    "top_titles_indices = np.argsort(-global_best_position, axis=0)\n",
    "top_titles = titles_df.iloc[top_titles_indices.flatten()]['Titles'].values.reshape(10, -1)\n",
    "\n",
    "print(top_titles_indices.shape)\n",
    "# Print the top 10 titles for each cluster\n",
    "# print(\"\\nTop titles for each cluster:\")\n",
    "# for cluster_index, cluster_titles in enumerate(top_titles):\n",
    "#     print(f\"Cluster {cluster_index + 1}:\")\n",
    "#     for title in cluster_titles:\n",
    "#         print(f\"  - {title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GREY WOLF OPTIMIZATION(GWO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class GreyWolf:\n",
    "    def __init__(self, num_clusters, num_features):\n",
    "        self.position = np.random.rand(num_clusters, num_features)\n",
    "        self.best_position = self.position.copy()\n",
    "        self.best_fitness = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gwo_fitness_function(data, wolves, num_clusters):\n",
    "    distances = np.zeros((len(data), num_clusters))\n",
    "\n",
    "    for i, wolf in enumerate(wolves):\n",
    "        for j in range(num_clusters):\n",
    "            distances[:, j] = np.linalg.norm(data - wolf.position[j], axis=1)\n",
    "\n",
    "        wolf_fitness = np.sum(np.min(distances, axis=1))\n",
    "\n",
    "        if wolf_fitness < wolf.best_fitness:\n",
    "            wolf.best_fitness = wolf_fitness\n",
    "            wolf.best_position = wolf.position.copy()\n",
    "\n",
    "    return wolves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gwo_update_position(wolves, a, num_clusters):\n",
    "    alpha, beta, delta = wolves[0], wolves[1], wolves[2]\n",
    "\n",
    "    for wolf in wolves:\n",
    "        r1, r2 = np.random.rand(), np.random.rand()\n",
    "        A1, A2, A3 = 2 * a * r1 - a, 2 * a * r2 - a, 2 * r2\n",
    "\n",
    "        D_alpha = np.abs(A1 * alpha.position - wolf.position)\n",
    "        D_beta = np.abs(A2 * beta.position - wolf.position)\n",
    "        D_delta = np.abs(A3 * delta.position - wolf.position)\n",
    "\n",
    "        X1 = alpha.position - A1 * D_alpha\n",
    "        X2 = beta.position - A2 * D_beta\n",
    "        X3 = delta.position - A3 * D_delta\n",
    "\n",
    "        wolf.position = (X1 + X2 + X3) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gwo_initialize_wolves(num_wolves, num_clusters, num_features):\n",
    "    wolves = [GreyWolf(num_clusters, num_features) for _ in range(num_wolves)]\n",
    "    return wolves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data and set parameters\n",
    "data = np.loadtxt(\"./data/wikipedia_td.txt\")\n",
    "num_wolves = 3  # Number of wolves in the GWO algorithm\n",
    "num_clusters = 10\n",
    "num_features = data.shape[1]\n",
    "max_iterations = 100\n",
    "a = 2.0  # Parameter for GWO algorithm\n",
    "\n",
    "# Initialize wolves\n",
    "wolves = gwo_initialize_wolves(num_wolves, num_clusters, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GWO Main Loop\n",
    "for iteration in range(max_iterations):\n",
    "    wolves = gwo_fitness_function(data, wolves, num_clusters)\n",
    "\n",
    "    # Update wolves' positions\n",
    "    gwo_update_position(wolves, a, num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 500)\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the best wolf's position\n",
    "global_best_position = wolves[np.argmin([wolf.best_fitness for wolf in wolves])].best_position\n",
    "\n",
    "\n",
    "# Load dictionary from CSV file\n",
    "dictionary_path = \"./data/dictionary.csv\"  # Replace with the actual path to your dictionary CSV file\n",
    "dictionary = {}\n",
    "\n",
    "with open(dictionary_path, \"r\", encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for index, word in enumerate(reader):\n",
    "        dictionary[index] = word[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[171 318 208 ... 453  24 149]\n",
      " [287 335 182 ... 381  74 348]\n",
      " [353 435 425 ... 205 225  83]\n",
      " ...\n",
      " [367 207 274 ... 203 292 484]\n",
      " [360 289 403 ...   0 278  68]\n",
      " [454 312  20 ... 110 477  18]]\n",
      "Top words for each cluster (GWO):\n",
      "Cluster 1: writing, annual, personal, version, argue, system, sometimes, local, website, cause\n",
      "Cluster 2: fellow, radio, seek, university, request, track, article, travel, conclude, cross\n",
      "Cluster 3: private, church, reviews, local, president, spend, decline, achieve, white, fail\n",
      "Cluster 4: significant, addition, minute, series, reference, debut, develop, website, plan, effect\n",
      "Cluster 5: accord, suffer, miss, executive, book, decline, commercial, website, debut, rest\n",
      "Cluster 6: san, suggest, central, bill, offer, game, total, canada, manager, reach\n",
      "Cluster 7: lead, occur, set, boy, war, private, mixed, ability, wear, sense\n",
      "Cluster 8: pick, promote, style, attention, history, king, activity, publish, produce, originally\n",
      "Cluster 9: mention, meeting, guest, participate, require, raise, popular, entire, claim, success\n",
      "Cluster 10: make, heart, report, director, local, location, respectively, multiple, america, pay\n"
     ]
    }
   ],
   "source": [
    "# Print the words corresponding to the top 10 entries of the global best position\n",
    "sorted_indices = np.argsort(-global_best_position, axis=1)\n",
    "print(sorted_indices)\n",
    "top_words = [[dictionary[idx] for idx in row[:10]] for row in sorted_indices]\n",
    "print(\"Top words for each cluster (GWO):\")\n",
    "for cluster_index, words in enumerate(top_words):\n",
    "    print(f\"Cluster {cluster_index + 1}: {', '.join(words)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load titles from CSV file\n",
    "titles_path = \"./data/titles.csv\"  # Replace with the actual path to your titles CSV file\n",
    "titles_df = pd.read_csv(titles_path, header=None, names=['Titles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6 1 7 ... 9 1 4]\n",
      " [9 4 6 ... 3 3 0]\n",
      " [0 2 2 ... 6 0 8]\n",
      " ...\n",
      " [3 8 5 ... 1 2 1]\n",
      " [5 0 8 ... 0 5 7]\n",
      " [8 9 4 ... 4 4 9]]\n",
      "Top 10 titles for each cluster:\n",
      "Cluster 1: Rodrigo_Duterte, Cam_Newton, Cam_Newton, Rodrigo_Duterte, Rodrigo_Duterte, Cam_Newton, Cam_Newton, Rodrigo_Duterte, Rodrigo_Duterte, Cam_Newton\n",
      "Cluster 2: Cam_Newton, Cam_Newton, Rodrigo_Duterte, Cam_Newton, Rodrigo_Duterte, Cam_Newton, Cam_Newton, Rodrigo_Duterte, Cam_Newton, Cam_Newton\n",
      "Cluster 3: Rodrigo_Duterte, Cam_Newton, Cam_Newton, Cam_Newton, Crimson_Peak, Cam_Newton, Rodrigo_Duterte, Cam_Newton, Crimson_Peak, Cam_Newton\n",
      "Cluster 4: Cam_Newton, Rodrigo_Duterte, Rodrigo_Duterte, Cam_Newton, Cam_Newton, Cam_Newton, Cam_Newton, Cam_Newton, Cam_Newton, Rodrigo_Duterte\n",
      "Cluster 5: Cam_Newton, Rodrigo_Duterte, Cam_Newton, Rodrigo_Duterte, Cam_Newton, Cam_Newton, Rodrigo_Duterte, Crimson_Peak, Cam_Newton, Rodrigo_Duterte\n",
      "Cluster 6: Cam_Newton, Cam_Newton, Cam_Newton, Crimson_Peak, Rodrigo_Duterte, Rodrigo_Duterte, Cam_Newton, Cam_Newton, Cam_Newton, Cam_Newton\n",
      "Cluster 7: Cam_Newton, Cam_Newton, Cam_Newton, Cam_Newton, Cam_Newton, Rodrigo_Duterte, Rodrigo_Duterte, Cam_Newton, Rodrigo_Duterte, Cam_Newton\n",
      "Cluster 8: Cam_Newton, Crimson_Peak, Rodrigo_Duterte, Rodrigo_Duterte, Cam_Newton, Cam_Newton, Cam_Newton, Cam_Newton, Cam_Newton, Crimson_Peak\n",
      "Cluster 9: Rodrigo_Duterte, Rodrigo_Duterte, Crimson_Peak, Cam_Newton, Cam_Newton, Rodrigo_Duterte, Cam_Newton, Rodrigo_Duterte, Cam_Newton, Rodrigo_Duterte\n",
      "Cluster 10: Crimson_Peak, Cam_Newton, Cam_Newton, Cam_Newton, Cam_Newton, Crimson_Peak, Crimson_Peak, Cam_Newton, Rodrigo_Duterte, Cam_Newton\n"
     ]
    }
   ],
   "source": [
    "sorted_indices = np.argsort(-global_best_position, axis=0)\n",
    "print(sorted_indices)\n",
    "top_titles = [[titles[idx] for idx in row[:10]] for row in sorted_indices]\n",
    "print(\"Top 10 titles for each cluster:\")\n",
    "for cluster_index, titles in enumerate(top_titles):\n",
    "    print(f\"Cluster {cluster_index + 1}: {', '.join(titles)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
